{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cadee26-40c4-4938-b709-d1d695045daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your statement i am a girl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of articles: 1\n"
     ]
    }
   ],
   "source": [
    "# WAP to count the number of articles in a sentence.\n",
    "articles=['a','an','the']\n",
    "text=input(\"Enter your statement\")\n",
    "c=0\n",
    "sentence=text.split()\n",
    "for s in sentence:\n",
    "    if s in articles:\n",
    "        c+=1\n",
    "print(\"No of articles:\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1965d9-20d1-45dd-9150-7eabff41d64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a statement she is a girl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'is', 'a', 'girl']\n"
     ]
    }
   ],
   "source": [
    "#WAP to tokenize an input string.\n",
    "text=input(\"Enter a statement\")\n",
    "sentence=text.split()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccf552a4-dbd7-43d9-a3e8-7430106e4b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a word madam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palindrome\n"
     ]
    }
   ],
   "source": [
    "# WAP to check if a string is palandrome or not\n",
    "s=input(\"Enter a word\")\n",
    "i=0\n",
    "n=len(s)-1\n",
    "flag=True\n",
    "while i<n:\n",
    "    if s[i] != s[n]:\n",
    "        flag=False\n",
    "        break\n",
    "    else:\n",
    "        i+=1\n",
    "        n-=1\n",
    "if flag==True:\n",
    "    print(\"palindrome\")\n",
    "else:\n",
    "    print(\"not palindrome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4100ed6-dfa8-4517-92e8-260e644b368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a statement my name is ishita. ishita is a girl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my : 1\n",
      "name : 1\n",
      "is : 2\n",
      "ishita. : 1\n",
      "ishita : 1\n",
      "a : 1\n",
      "girl : 1\n"
     ]
    }
   ],
   "source": [
    "# WAP to enter a paragraph and split each sentence and cout frequency of each word.\n",
    "text=input(\"Enter a statement\")\n",
    "sent=text.split()\n",
    "list={}\n",
    "for w in sent:\n",
    "    if w in list:\n",
    "        list[w]+=1\n",
    "    else:\n",
    "        list[w]=1\n",
    "for w, c in list.items():\n",
    "    print(w,\":\",c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f3cf81c-f11d-488c-be7a-6352efcac864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a statement in hindi मैं एक लड़की हूँ \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['मैं', 'एक', 'लड़की', 'हूँ']\n"
     ]
    }
   ],
   "source": [
    "#WAP to tokenize a hindi input string.\n",
    "text=input(\"Enter a statement in hindi\")\n",
    "sent=text.split()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4ac0c48-1d2d-426c-a511-51c7404c4395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hundred twenty three\n"
     ]
    }
   ],
   "source": [
    "# WAP to input a number and print it in textual format (eg. 516234 will be Five lacs sixteen thousand two hundred and thirty four)\n",
    "ones = [\"\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "teens = [\"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "         \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\"]\n",
    "tens = [\"\", \"\", \"twenty\", \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\"]\n",
    "\n",
    "def two_digit(n):\n",
    "    if n < 10:\n",
    "        return ones[n]\n",
    "    elif n < 20:\n",
    "        return teens[n-10]\n",
    "    else:\n",
    "        return tens[n//10] + \" \" + ones[n%10]\n",
    "\n",
    "def three_digit(n):\n",
    "    if n >= 100:\n",
    "        return ones[n//100] + \" hundred \" + two_digit(n%100)\n",
    "    else:\n",
    "        return two_digit(n)\n",
    "\n",
    "num = int(input(\"Enter number: \"))\n",
    "\n",
    "lakh = num // 100000\n",
    "num %= 100000\n",
    "\n",
    "thousand = num // 1000\n",
    "num %= 1000\n",
    "\n",
    "hundred_part = num\n",
    "\n",
    "result = \"\"\n",
    "\n",
    "if lakh:\n",
    "    result += two_digit(lakh) + \" lakh \"\n",
    "if thousand:\n",
    "    result += two_digit(thousand) + \" thousand \"\n",
    "if hundred_part:\n",
    "    result += three_digit(hundred_part)\n",
    "\n",
    "print(result.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10b6d6c6-e1af-4184-9e85-ea8f1600542f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "संख्या दर्ज करें:  12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "बारह\n"
     ]
    }
   ],
   "source": [
    "#WAP to input a number and print it in textual format in Hindi\n",
    "ones = [\"\", \"एक\", \"दो\", \"तीन\", \"चार\", \"पांच\", \"छह\", \"सात\", \"आठ\", \"नौ\"]\n",
    "teens = [\"दस\", \"ग्यारह\", \"बारह\", \"तेरह\", \"चौदह\", \"पंद्रह\",\n",
    "         \"सोलह\", \"सत्रह\", \"अठारह\", \"उन्नीस\"]\n",
    "tens = [\"\", \"\", \"बीस\", \"तीस\", \"चालीस\", \"पचास\", \"साठ\", \"सत्तर\", \"अस्सी\", \"नब्बे\"]\n",
    "\n",
    "def two_digit(n):\n",
    "    if n < 10:\n",
    "        return ones[n]\n",
    "    elif n < 20:\n",
    "        return teens[n-10]\n",
    "    else:\n",
    "        return tens[n//10] + \" \" + ones[n%10]\n",
    "\n",
    "def three_digit(n):\n",
    "    if n >= 100:\n",
    "        return ones[n//100] + \" सौ \" + two_digit(n%100)\n",
    "    else:\n",
    "        return two_digit(n)\n",
    "\n",
    "num = int(input(\"संख्या दर्ज करें: \"))\n",
    "\n",
    "lakh = num // 100000\n",
    "num %= 100000\n",
    "\n",
    "thousand = num // 1000\n",
    "num %= 1000\n",
    "\n",
    "rest = num\n",
    "\n",
    "result = \"\"\n",
    "\n",
    "if lakh:\n",
    "    result += two_digit(lakh) + \" लाख \"\n",
    "if thousand:\n",
    "    result += two_digit(thousand) + \" हजार \"\n",
    "if rest:\n",
    "    result += three_digit(rest)\n",
    "\n",
    "print(result.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24bf1ccc-8119-4882-ae6b-fb0df2f4df84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter a text .];\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "#WAP to count the punctuations in a given string.\n",
    "punct=['!','@','#','$','%','^','&','*','(',')','-','_','=','+','{',']','[','}',':',';','\"','<',',','>','.','?','/']\n",
    "text=input(\"enter a text\")\n",
    "c=0\n",
    "for s in text:\n",
    "    if s in punct:\n",
    "        c+=1\n",
    "print(c)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bae97cb1-4e86-4b96-a891-2fd22d0b60a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter a text ईशिता ने खाना खाया \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#WAP to count the number of karakas in a given Hindi string.\n",
    "#WAP to count the punctuations in a given string.\n",
    "karak=['ने','को','से','केलिए','का','के','की','रा','रे','री','में','पर']\n",
    "text=input(\"enter a text\")\n",
    "text=text.split()\n",
    "c=0\n",
    "for s in text:\n",
    "    if s in karak:\n",
    "        c+=1\n",
    "print(c)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da0f49b5-e5a7-4b66-961a-5e8b191cea06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter a text amit kumar jain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A   K   jain\n"
     ]
    }
   ],
   "source": [
    "#WAP to input a name of a person in the format <First Name> <Middle Name> <Family Name> and print the initial of first and middle names with family name (Eg. Mohandas Karamchand Gandhi should be printed as MK Gandhi)\n",
    "text=input(\"enter a text\")\n",
    "text=text.split()\n",
    "if len(text)==3:\n",
    "    first=text[0][0].upper()\n",
    "    sec=text[1][0].upper()\n",
    "    third=text[2]\n",
    "    print(first,\" \",sec,\" \",third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "02f1d038-4347-46d5-9ec4-9e7c717c5dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter a statement Hi! How are you? I'm fine. Great.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi!', 'How are you?', \"I'm fine.\", 'Great.']\n"
     ]
    }
   ],
   "source": [
    "#1. Rule-based sentence splitter\n",
    "#* Write a function split_sentences(text) that splits into sentences using . ? ! as delimiters.\n",
    "#* Keep the delimiter attached to the sentence.\n",
    "#* Ignore extra spaces.\n",
    "#* Test on: \"Hi! How are you? I'm fine. Great.\"\n",
    "delimiters=['.','?','!']\n",
    "sen=[]\n",
    "cur=\"\"\n",
    "text=input(\"enter a statement\")\n",
    "for ch in text:\n",
    "    cur+=ch\n",
    "    if ch in delimiters:\n",
    "        sen.append(cur.strip())\n",
    "        cur=\"\"\n",
    "if cur.strip():\n",
    "    sen.append(cur.strip())\n",
    "print(sen)\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a598f45d-ec8f-4b8e-8855-fad9c1addceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a statement:  Dr. Sharma arrived at 5 p.m. He left soon after\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dr. Sharma arrived at 5 p.', 'm.', 'He left soon after']\n"
     ]
    }
   ],
   "source": [
    "#Handle abbreviations in sentence splitting\n",
    "#* Improve ####1 so it doesn’t split after common abbreviations:\n",
    "#* Mr., Mrs., Dr., Prof., etc., e.g., i.e.\n",
    "#* Test on: \"Dr. Sharma arrived at 5 p.m. He left soon after.\"\n",
    "def split_sentences(text):\n",
    "    abb = ['Mr.', 'Mrs.', 'Dr.', 'Prof.', 'etc.', 'e.g.', 'i.e.']\n",
    "    sentences = []\n",
    "    cur = \"\"\n",
    "\n",
    "    for ch in text:\n",
    "        cur += ch\n",
    "\n",
    "        if ch in \".?!\":\n",
    "            last_word = cur.strip().split()[-1]\n",
    "\n",
    "            if last_word in abb:\n",
    "                continue\n",
    "            else:\n",
    "                sentences.append(cur.strip())\n",
    "                cur = \"\"\n",
    "\n",
    "    if cur.strip():\n",
    "        sentences.append(cur.strip())\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "text = input(\"Enter a statement: \")\n",
    "print(split_sentences(text))\n",
    "\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b0b93ec-0ea1-4405-9c21-4f42d532b97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a statement:   He said, \"Really?\" Then left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He said, \"Really?\" Then left.']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a statement:  sj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sj']\n"
     ]
    }
   ],
   "source": [
    "#Sentence splitting with quotes and parentheses Split sentences correctly when punctuation is inside quotes/parentheses:\n",
    "punct=['!','@','#','$','%','^','&','*','(',')','-','_','=','+','{',']','[','}',':',';','\"','<',',','>','.','?','/']\n",
    "def split_sentences(text):\n",
    "    sentences = []\n",
    "    current = \"\"\n",
    "\n",
    "    in_quotes = False\n",
    "    paren_count = 0\n",
    "\n",
    "    for ch in text:\n",
    "        current += ch\n",
    "\n",
    "        # Toggle quote state\n",
    "        if ch == '\"':\n",
    "            in_quotes = not in_quotes\n",
    "\n",
    "        # Track parentheses depth\n",
    "        elif ch == '(':\n",
    "            paren_count += 1\n",
    "        elif ch == ')':\n",
    "            if paren_count > 0:\n",
    "                paren_count -= 1\n",
    "\n",
    "        # Sentence boundary\n",
    "        if ch in '.?!' and not in_quotes and paren_count == 0:\n",
    "            sentences.append(current.strip())\n",
    "            current = \"\"\n",
    "\n",
    "    # Add leftover text\n",
    "    if current.strip():\n",
    "        sentences.append(current.strip())\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "text = input(\"Enter a statement: \")\n",
    "print(split_sentences(text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "21bdb46c-9175-45aa-93c4-5236f899a4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text:  Hello, world!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!']\n"
     ]
    }
   ],
   "source": [
    "#Word tokenizer with punctuation separation\n",
    "#Write tokenize_words(text) that returns tokens where punctuation becomes separate tokens.\n",
    "#\"Hello, world!\" → [\"Hello\", \",\", \"world\", \"!\"]\n",
    "#Handle ... as a single token or three tokens (your choice, but be consistent).\n",
    "def tokenize_words(text):\n",
    "    tokens = []\n",
    "    current = \"\"\n",
    "    i = 0\n",
    "\n",
    "    while i < len(text):\n",
    "        ch = text[i]\n",
    "\n",
    "        # Handle ellipsis ...\n",
    "        if text[i:i+3] == \"...\":\n",
    "            if current:\n",
    "                tokens.append(current)\n",
    "                current = \"\"\n",
    "            tokens.append(\"...\")\n",
    "            i += 3\n",
    "            continue\n",
    "\n",
    "        # If alphanumeric → build word\n",
    "        if ch.isalnum():\n",
    "            current += ch\n",
    "\n",
    "        else:\n",
    "            # End current word\n",
    "            if current:\n",
    "                tokens.append(current)\n",
    "                current = \"\"\n",
    "\n",
    "            # Add punctuation as token (ignore spaces)\n",
    "            if not ch.isspace():\n",
    "                tokens.append(ch)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # Add last word\n",
    "    if current:\n",
    "        tokens.append(current)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "text = input(\"Enter text: \")\n",
    "print(tokenize_words(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1fc54234-4134-424f-9b91-1f79e7bea6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text:  Ram's book\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ram', \"'s\", 'book']\n"
     ]
    }
   ],
   "source": [
    "# Contractions and possessives\n",
    "#Extend your word tokenizer to handle:\n",
    "#\"don't\" as [\"do\", \"n't\"] or [\"don't\"] (choose a scheme)\n",
    "#\"Ram's book\" as [\"Ram\", \"'s\", \"book\"] or [\"Ram's\", \"book\"]\n",
    "#Then document your rule.\n",
    "def tokenize_words(text):\n",
    "    tokens = []\n",
    "    current = \"\"\n",
    "    i = 0\n",
    "\n",
    "    while i < len(text):\n",
    "\n",
    "        # Handle ellipsis\n",
    "        if text[i:i+3] == \"...\":\n",
    "            if current:\n",
    "                tokens.append(current)\n",
    "                current = \"\"\n",
    "            tokens.append(\"...\")\n",
    "            i += 3\n",
    "            continue\n",
    "\n",
    "        ch = text[i]\n",
    "\n",
    "        if ch.isalnum():\n",
    "            current += ch\n",
    "\n",
    "        elif ch == \"'\":\n",
    "            # Look ahead for contraction or possessive\n",
    "            if current:\n",
    "                j = i + 1\n",
    "                suffix = \"\"\n",
    "                while j < len(text) and text[j].isalpha():\n",
    "                    suffix += text[j]\n",
    "                    j += 1\n",
    "\n",
    "                # Common contraction patterns\n",
    "                if suffix.lower() in [\"s\", \"t\", \"re\", \"ve\", \"ll\", \"d\", \"m\"]:\n",
    "                    tokens.append(current)\n",
    "                    tokens.append(\"'\" + suffix)\n",
    "                    current = \"\"\n",
    "                    i = j\n",
    "                    continue\n",
    "                else:\n",
    "                    current += ch\n",
    "            else:\n",
    "                tokens.append(ch)\n",
    "\n",
    "        else:\n",
    "            if current:\n",
    "                tokens.append(current)\n",
    "                current = \"\"\n",
    "\n",
    "            if not ch.isspace():\n",
    "                tokens.append(ch)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    if current:\n",
    "        tokens.append(current)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "text = input(\"Enter text: \")\n",
    "print(tokenize_words(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "48df5798-3967-4338-a67f-3a3902110574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter text:  3.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3.14']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize numbers, dates, and decimals\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "\n",
    "    # Regex pattern for special numeric tokens\n",
    "    pattern = r'''\n",
    "        ₹\\d+(?:,\\d{2,3})*(?:\\.\\d+)? |     # ₹25.50 or ₹1,00,000\n",
    "        \\d{1,2}/\\d{1,2}/\\d{2,4} |         # dates like 12/01/2026\n",
    "        \\d+(?:,\\d{2,3})+(?:\\.\\d+)? |      # 1,00,000\n",
    "        \\d+\\.\\d+ |                        # decimals like 3.14\n",
    "        \\w+ |                             # words\n",
    "        [^\\w\\s]                           # punctuation\n",
    "    '''\n",
    "\n",
    "    tokens = re.findall(pattern, text, re.VERBOSE)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Test\n",
    "text = input(\"Enter text: \")\n",
    "print(tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a7607af-3350-4e0a-b2dd-02fa8adea2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['भ', 'रत', 'एक', 'मह', 'न', 'द', 'श', 'ह', '।', 'क', 'य', 'आप', 'सहमत', 'ह', '?']\n"
     ]
    }
   ],
   "source": [
    "#Tokenization for Indic text (Hindi/Devanagari or and Indian Language of your choice)\n",
    "import re\n",
    "\n",
    "def hindi_tokenize(text):\n",
    "    # Define Hindi punctuation\n",
    "    punct = r\"[।,?!]\"\n",
    "\n",
    "    tokens = []\n",
    "\n",
    "    # Split by whitespace first\n",
    "    parts = text.split()\n",
    "\n",
    "    for part in parts:\n",
    "        # Split punctuation while keeping it\n",
    "        split_parts = re.findall(r'\\w+|[।,?!]', part, re.UNICODE)\n",
    "        tokens.extend(split_parts)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Test sentence\n",
    "text = \"भारत एक महान देश है। क्या आप सहमत हैं?\"\n",
    "print(hindi_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d5154c5-2533-4c1c-9d2b-067fdcbec93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.619\n",
      "Recall: 0.833\n",
      "F1: 0.711\n",
      "\n",
      "Worst 5 examples:\n",
      "\n",
      "Text: क्या आप ठीक हैं?\n",
      "Gold: ['क्या', 'आप', 'ठीक', 'हैं', '?']\n",
      "Pred: ['क', '्', 'य', 'ा', 'आप', 'ठ', 'ी', 'क', 'ह', 'ै', 'ं', '?']\n",
      "Mismatch: 13\n",
      "----------------------------------------\n",
      "Text: भारत महान है।\n",
      "Gold: ['भारत', 'महान', 'है', '।']\n",
      "Pred: ['भ', 'ा', 'रत', 'मह', 'ा', 'न', 'ह', 'ै', '।']\n",
      "Mismatch: 11\n",
      "----------------------------------------\n",
      "Text: Numbers 1,00,000 are big.\n",
      "Gold: ['Numbers', '1,00,000', 'are', 'big', '.']\n",
      "Pred: ['Numbers', '1', ',', '00', ',', '000', 'are', 'big', '.']\n",
      "Mismatch: 6\n",
      "----------------------------------------\n",
      "Text: Date: 12/01/2026\n",
      "Gold: ['Date', ':', '12/01/2026']\n",
      "Pred: ['Date', ':', '12', '/', '01', '/', '2026']\n",
      "Mismatch: 6\n",
      "----------------------------------------\n",
      "Text: Price ₹25.50 only.\n",
      "Gold: ['Price', '₹25.50', 'only', '.']\n",
      "Pred: ['Price', '₹', '25', '.', '50', 'only', '.']\n",
      "Mismatch: 5\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    " #Build a tokenizer evaluation script \n",
    "#Given a small “gold” tokenized dataset (you create 20 sentences):\n",
    "# Compute token-level precision/recall/F1 for your tokenizer vs gold.\n",
    "# Print 5 worst examples (highest token mismatch).\n",
    "from collections import Counter\n",
    "\n",
    "# -------------------------------\n",
    "# Example tokenizer (replace with yours)\n",
    "# -------------------------------\n",
    "import re\n",
    "\n",
    "def my_tokenizer(text):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Gold dataset (20 sentences)\n",
    "# -------------------------------\n",
    "gold_data = [\n",
    "    (\"Hello, world!\", [\"Hello\", \",\", \"world\", \"!\"]),\n",
    "    (\"I don't know.\", [\"I\", \"do\", \"n't\", \"know\", \".\"]),\n",
    "    (\"Ram's book is here.\", [\"Ram\", \"'s\", \"book\", \"is\", \"here\", \".\"]),\n",
    "    (\"Value is 3.14.\", [\"Value\", \"is\", \"3.14\", \".\"]),\n",
    "    (\"Date: 12/01/2026\", [\"Date\", \":\", \"12/01/2026\"]),\n",
    "    (\"Price ₹25.50 only.\", [\"Price\", \"₹25.50\", \"only\", \".\"]),\n",
    "    (\"He said hello.\", [\"He\", \"said\", \"hello\", \".\"]),\n",
    "    (\"Wait... really?\", [\"Wait\", \"...\", \"really\", \"?\"]),\n",
    "    (\"Open the door!\", [\"Open\", \"the\", \"door\", \"!\"]),\n",
    "    (\"Close it now.\", [\"Close\", \"it\", \"now\", \".\"]),\n",
    "    (\"Numbers 1,00,000 are big.\", [\"Numbers\", \"1,00,000\", \"are\", \"big\", \".\"]),\n",
    "    (\"Yes or no?\", [\"Yes\", \"or\", \"no\", \"?\"]),\n",
    "    (\"This is a test.\", [\"This\", \"is\", \"a\", \"test\", \".\"]),\n",
    "    (\"Tokenization is fun!\", [\"Tokenization\", \"is\", \"fun\", \"!\"]),\n",
    "    (\"Check punctuation.\", [\"Check\", \"punctuation\", \".\"]),\n",
    "    (\"Email me now.\", [\"Email\", \"me\", \"now\", \".\"]),\n",
    "    (\"भारत महान है।\", [\"भारत\", \"महान\", \"है\", \"।\"]),\n",
    "    (\"क्या आप ठीक हैं?\", [\"क्या\", \"आप\", \"ठीक\", \"हैं\", \"?\"]),\n",
    "    (\"Mix Hindi and English.\", [\"Mix\", \"Hindi\", \"and\", \"English\", \".\"]),\n",
    "    (\"Final example!\", [\"Final\", \"example\", \"!\"])\n",
    "]\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation\n",
    "# -------------------------------\n",
    "total_tp = total_fp = total_fn = 0\n",
    "mismatch_scores = []\n",
    "\n",
    "for text, gold_tokens in gold_data:\n",
    "\n",
    "    pred_tokens = my_tokenizer(text)\n",
    "\n",
    "    gold_counter = Counter(gold_tokens)\n",
    "    pred_counter = Counter(pred_tokens)\n",
    "\n",
    "    tp = sum((gold_counter & pred_counter).values())\n",
    "    fp = sum((pred_counter - gold_counter).values())\n",
    "    fn = sum((gold_counter - pred_counter).values())\n",
    "\n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "\n",
    "    mismatch = fp + fn\n",
    "    mismatch_scores.append((mismatch, text, gold_tokens, pred_tokens))\n",
    "\n",
    "\n",
    "# Metrics\n",
    "precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) else 0\n",
    "recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "\n",
    "print(\"Precision:\", round(precision, 3))\n",
    "print(\"Recall:\", round(recall, 3))\n",
    "print(\"F1:\", round(f1, 3))\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Worst 5 examples\n",
    "# -------------------------------\n",
    "print(\"\\nWorst 5 examples:\\n\")\n",
    "\n",
    "for mismatch, text, gold, pred in sorted(mismatch_scores, reverse=True)[:5]:\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Gold:\", gold)\n",
    "    print(\"Pred:\", pred)\n",
    "    print(\"Mismatch:\", mismatch)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80b76946-fddd-46d5-af09-29c5661306d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n",
      " ['Natural Language Processing is fun!', 'This pipeline cleans text, removes noise,\\nand builds a vocabulary.', 'It works well for experiments.']\n",
      "\n",
      "Tokens per sentence:\n",
      "['natural', 'language', 'processing', 'fun']\n",
      "['pipeline', 'cleans', 'text', 'removes', 'noise', 'builds', 'vocabulary']\n",
      "['works', 'well', 'experiments']\n",
      "\n",
      "Vocabulary:\n",
      "Counter({'natural': 1, 'language': 1, 'processing': 1, 'fun': 1, 'pipeline': 1, 'cleans': 1, 'text': 1, 'removes': 1, 'noise': 1, 'builds': 1, 'vocabulary': 1, 'works': 1, 'well': 1, 'experiments': 1})\n"
     ]
    }
   ],
   "source": [
    "#Tokenization-aware text preprocessing pipeline\n",
    "# Create a pipeline function:\n",
    "# preprocess(text) -> {sentences: [...], tokens_per_sentence: [...], vocab: Counter}\n",
    "# Requirements:\n",
    " #   - sentence tokenize\n",
    "  #  - word tokenize each sentence\n",
    "   # - remove stopwords (optional)\n",
    "    #- normalize (lowercase, unicode normalize, strip extra punctuation)\n",
    "#output vocab frequency\n",
    "# Run it on a short paragraph and show outputs neatly.\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "\n",
    "# Simple stopword list (extend if needed)\n",
    "STOPWORDS = {\n",
    "    \"the\", \"is\", \"a\", \"an\", \"and\", \"or\", \"to\", \"of\", \"in\",\n",
    "    \"on\", \"for\", \"with\", \"at\", \"by\", \"it\", \"this\", \"that\"\n",
    "}\n",
    "\n",
    "# Sentence tokenizer\n",
    "def sentence_tokenize(text):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return sentences\n",
    "\n",
    "# Word tokenizer\n",
    "def word_tokenize(sentence):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", sentence)\n",
    "\n",
    "# Normalization\n",
    "def normalize(token):\n",
    "    token = unicodedata.normalize(\"NFKC\", token)\n",
    "    token = token.lower()\n",
    "    token = re.sub(r\"^[^\\w]+|[^\\w]+$\", \"\", token)  # strip punctuation edges\n",
    "    return token\n",
    "\n",
    "# Pipeline\n",
    "def preprocess(text, remove_stopwords=True):\n",
    "\n",
    "    sentences = sentence_tokenize(text)\n",
    "\n",
    "    tokens_per_sentence = []\n",
    "    vocab = Counter()\n",
    "\n",
    "    for sent in sentences:\n",
    "        tokens = word_tokenize(sent)\n",
    "\n",
    "        processed_tokens = []\n",
    "        for tok in tokens:\n",
    "            norm = normalize(tok)\n",
    "\n",
    "            if not norm:\n",
    "                continue\n",
    "\n",
    "            if remove_stopwords and norm in STOPWORDS:\n",
    "                continue\n",
    "\n",
    "            processed_tokens.append(norm)\n",
    "            vocab[norm] += 1\n",
    "\n",
    "        tokens_per_sentence.append(processed_tokens)\n",
    "\n",
    "    return {\n",
    "        \"sentences\": sentences,\n",
    "        \"tokens_per_sentence\": tokens_per_sentence,\n",
    "        \"vocab\": vocab\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Example run\n",
    "# -----------------------\n",
    "\n",
    "text = \"\"\"\n",
    "Natural Language Processing is fun! This pipeline cleans text, removes noise,\n",
    "and builds a vocabulary. It works well for experiments.\n",
    "\"\"\"\n",
    "\n",
    "result = preprocess(text)\n",
    "\n",
    "print(\"Sentences:\\n\", result[\"sentences\"])\n",
    "print(\"\\nTokens per sentence:\")\n",
    "for t in result[\"tokens_per_sentence\"]:\n",
    "    print(t)\n",
    "\n",
    "print(\"\\nVocabulary:\")\n",
    "print(result[\"vocab\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a77c18f9-732a-4986-9120-cdd51901620a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 2, 'love': 2, 'nlp': 1, 'and': 1, 'coding': 1})\n"
     ]
    }
   ],
   "source": [
    "#WAP to unigram words in a given corpus\n",
    "from collections import Counter\n",
    "def unigram(corpus):\n",
    "    words=corpus.lower().split()\n",
    "    return Counter(words)\n",
    "text=\"I love NLP and I love coding\"\n",
    "print(unigram(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "775acf0d-e9a9-472e-bf66-0528d6bb6de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter corpus:  in am a girl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('am', 'girl')]\n"
     ]
    }
   ],
   "source": [
    "#WAP to generate bigrams for a given corpus after eliminating stopwords and punctuations\n",
    "import re\n",
    "\n",
    "# Simple stopword list (extend if needed)\n",
    "STOPWORDS = {\n",
    "    \"the\", \"is\", \"a\", \"an\", \"and\", \"or\", \"to\", \"of\", \"in\",\n",
    "    \"on\", \"for\", \"with\", \"at\", \"by\", \"it\", \"this\", \"that\"\n",
    "}\n",
    "\n",
    "def generate_bigrams(text):\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "\n",
    "    # Tokenize\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered = [w for w in words if w not in STOPWORDS]\n",
    "\n",
    "    # Generate bigrams\n",
    "    bigrams = [(filtered[i], filtered[i+1]) for i in range(len(filtered)-1)]\n",
    "\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "# Input\n",
    "corpus = input(\"Enter corpus: \")\n",
    "\n",
    "# Output\n",
    "print(\"Bigrams:\", generate_bigrams(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc714fa3-ede2-47fa-9d06-fbadca4f7ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams: [('natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'fun'), ('language', 'models', 'learn'), ('models', 'learn', 'patterns')]\n"
     ]
    }
   ],
   "source": [
    "#AP to generate trigrams from a collection of sentences.\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "def generate_trigrams(sentences):\n",
    "    trigrams = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        words = tokenize(sent)\n",
    "        for i in range(len(words) - 2):\n",
    "            trigrams.append((words[i], words[i+1], words[i+2]))\n",
    "\n",
    "    return trigrams\n",
    "\n",
    "\n",
    "# Example input\n",
    "sentences = [\n",
    "    \"Natural language processing is fun\",\n",
    "    \"Language models learn patterns\"\n",
    "]\n",
    "\n",
    "print(\"Trigrams:\", generate_trigrams(sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a9bf2cb-90a0-47a3-9ee0-cffae97e8c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Frequencies:\n",
      "('natural', 'language', 'processing') : 1\n",
      "('language', 'processing', 'is') : 1\n",
      "('processing', 'is', 'fun') : 1\n",
      "('language', 'models', 'learn') : 1\n",
      "('models', 'learn', 'patterns') : 1\n"
     ]
    }
   ],
   "source": [
    "#WAP to calculate frequencies for the generated trigrams.\n",
    "from collections import Counter\n",
    "\n",
    "trigrams = generate_trigrams(sentences)\n",
    "\n",
    "freq = Counter(trigrams)\n",
    "\n",
    "print(\"Trigram Frequencies:\")\n",
    "for tri, count in freq.items():\n",
    "    print(tri, \":\", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e362954f-ac52-4993-ab66-10628a9407c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trigram Probabilities:\n",
      "('natural', 'language', 'processing') : 1.0\n",
      "('language', 'processing', 'is') : 1.0\n",
      "('processing', 'is', 'fun') : 1.0\n",
      "('language', 'models', 'learn') : 1.0\n",
      "('models', 'learn', 'patterns') : 1.0\n"
     ]
    }
   ],
   "source": [
    "#WAP to generate a triram model of a collection of sentences.\n",
    "#P(w3|w1,w2) = C(w1,w2,w3)/C(w1,w2)\n",
    "def trigram_model(sentences):\n",
    "    trigram_counts = Counter()\n",
    "    bigram_counts = Counter()\n",
    "\n",
    "    for sent in sentences:\n",
    "        words = tokenize(sent)\n",
    "\n",
    "        for i in range(len(words) - 2):\n",
    "            trigram = (words[i], words[i+1], words[i+2])\n",
    "            bigram = (words[i], words[i+1])\n",
    "\n",
    "            trigram_counts[trigram] += 1\n",
    "            bigram_counts[bigram] += 1\n",
    "\n",
    "    model = {}\n",
    "\n",
    "    for (w1, w2, w3), count in trigram_counts.items():\n",
    "        prob = count / bigram_counts[(w1, w2)]\n",
    "        model[(w1, w2, w3)] = prob\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = trigram_model(sentences)\n",
    "\n",
    "print(\"\\nTrigram Probabilities:\")\n",
    "for tri, prob in model.items():\n",
    "    print(tri, \":\", round(prob, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76787b6c-bd6f-470a-b3ee-bbbc55ee60e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
