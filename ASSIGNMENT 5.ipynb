{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b6d0a-e865-4fc2-94f8-e743b1036388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assignment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08f40d9-2e11-4d27-a2b4-300fb4d03153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentences I am a girl. My name is ishita. I live in bhopal.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 3\n"
     ]
    }
   ],
   "source": [
    "#WAP to count the number of sentences in an input text.\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "text=input(\"Enter sentences\")\n",
    "text=sent_tokenize(text)\n",
    "c=0\n",
    "for i in text:\n",
    "    c+=1\n",
    "print(\"Number of sentences:\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afce22b6-e779-4a25-b027-fefa1f53db04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentences I am a girl. My name is ishita. I live in bhopal.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0\n",
      "Sentence I am a girl.\n",
      "Length of sentence 5\n",
      "Index 1\n",
      "Sentence My name is ishita.\n",
      "Length of sentence 5\n",
      "Index 2\n",
      "Sentence I live in bhopal.\n",
      "Length of sentence 5\n"
     ]
    }
   ],
   "source": [
    "#WAP to print each sentence along with its sentence index (1..N) and sentence length (number of tokens).\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "text=input(\"Enter sentences\")\n",
    "text=sent_tokenize(text)\n",
    "c=0\n",
    "for i in text:\n",
    "    print(\"Index\",c)\n",
    "    c+=1\n",
    "    print(\"Sentence\",i)\n",
    "    print(\"Length of sentence\",len(word_tokenize(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d822597-8106-45dd-bf71-6f63584f9c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentences:  I am a girl. My name is ishita. I live in Bhopal. I study at banasthali vidyapith. I like food.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 0: Token count = 5\n",
      "Sentence 1: Token count = 5\n",
      "Sentence 2: Token count = 5\n",
      "Sentence 3: Token count = 6\n",
      "Sentence 4: Token count = 4\n",
      "\n",
      "Longest sentence:\n",
      "I study at banasthali vidyapith.\n",
      "\n",
      "Shortest sentence:\n",
      "I like food.\n"
     ]
    }
   ],
   "source": [
    "#WAP to find the longest and shortest sentence (by token count) and print them.\n",
    "import nltk\n",
    "import math\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = input(\"Enter sentences: \")\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "lengths = []\n",
    "\n",
    "# Calculate token count\n",
    "for i, sent in enumerate(sentences):\n",
    "    token_count = len(word_tokenize(sent))\n",
    "    lengths.append(token_count)\n",
    "    print(f\"Sentence {i}: Token count = {token_count}\")\n",
    "\n",
    "# Find longest & shortest\n",
    "longest_index = lengths.index(max(lengths))\n",
    "shortest_index = lengths.index(min(lengths))\n",
    "\n",
    "print(\"\\nLongest sentence:\")\n",
    "print(sentences[longest_index])\n",
    "\n",
    "print(\"\\nShortest sentence:\")\n",
    "print(sentences[shortest_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8676dba-d3cf-461d-9d2b-d3a54b6f96ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentences I am a girl. My name is ishita. I live in Bhopal. I study at banasthali vidyapith. I like food.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 : 1\n",
      "10 : 2\n",
      "15 : 3\n",
      "21 : 4\n",
      "25 : 5\n",
      "Average: 5.0\n"
     ]
    }
   ],
   "source": [
    "# WAP to compute the average sentence length (tokens per sentence).\n",
    "#WAP to print each sentence along with its sentence index (1..N) and sentence length (number of tokens).\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "text=input(\"Enter sentences\")\n",
    "text=sent_tokenize(text)\n",
    "c=0\n",
    "l=0\n",
    "for i in text:\n",
    "    c+=1\n",
    "    token_count = len(word_tokenize(i))\n",
    "    l+=token_count\n",
    "    print(l,\":\",c)\n",
    "print(\"Average:\",l/c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98a7758d-05e1-4441-98e3-d2a9ff36b2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter sentences I am a girl. My name is ishita. I live in Bhopal. I study at banasthali vidyapith. I like food. How are you? Great!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions:  1  Statements: 5  Exclamations: 1\n"
     ]
    }
   ],
   "source": [
    "# WAP to count how many sentences are questions, exclamations, and statements (based on ending punctuation ? ! .).\n",
    "q=0,s=0,e=0\n",
    "import nltk\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "text=input(\"Enter sentences\")\n",
    "text=sent_tokenize(text)\n",
    "for i in text:\n",
    "    if i.endswith('.'):\n",
    "        s+=1\n",
    "    elif i.endswith('!'):\n",
    "        e+=1\n",
    "    else:\n",
    "        q+=1\n",
    "print(\"Questions: \",q,\" Statements:\",s,\" Exclamations:\",e)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4c5b625-ef47-4eec-99e5-196ced348c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter statements: I am a girl. My name is ishita. I live in Bhopal. I study at banasthali vidyapith. I like food. How are you? Great! Eating, playing, happiness, fairly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "My\n",
      "I\n",
      "Bhopal\n",
      "I\n",
      "study\n",
      "I\n",
      "How\n",
      "Great\n",
      "Eating\n",
      "playing\n",
      "happiness\n",
      "fairly\n",
      "Different tokens 13\n"
     ]
    }
   ],
   "source": [
    "#WAP to count the number of tokens whose lemma is different from the surface form.\n",
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "text=input(\"Enter statements:\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "count=0\n",
    "for w in tokens:\n",
    "    if(ps.stem(w)!=w):\n",
    "        count+=1\n",
    "        print(w)\n",
    "print(\"Different tokens\",count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57e143dd-9550-4dc8-b6ff-53a5e2040670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter statements: I am a girl. My name is ishita. I live in Bhopal. I study at banasthali vidyapith. I like food. How are you? Great! Eating, playing, happiness, fairly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem Diversity: 1.0\n"
     ]
    }
   ],
   "source": [
    "#WAP to compute the lemma diversity: #unique lemmas / #unique tokens (ignore punctuation + spaces).\n",
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "text=input(\"Enter statements:\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "stems=[ps.stem(w) for w in tokens]\n",
    "print(\"Stem Diversity:\",len(set(stems))/len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a47630d6-67ec-4db0-a608-f2ea25d46c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter statements: I am a girl. My name is ishita. I live in Bhopal. I study at banasthali vidyapith. I like food. How are you? Great! Eating, playing, happiness, fairly. She is a good girl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "8\n",
      "7\n"
     ]
    }
   ],
   "source": [
    " #WAP to count the number of NOUN, PROPN, PRON separately and total “noun-like” count.\n",
    "import nltk\n",
    "from nltk import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "text=input(\"Enter statements:\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "tags=nltk.pos_tag(tokens)\n",
    "n=0\n",
    "p=0\n",
    "pn=0\n",
    "for w,t in tags:\n",
    "    if t.startswith('NNP'):\n",
    "        pn+=1\n",
    "    elif t.startswith('NN'):\n",
    "        n+=1\n",
    "    elif t.startswith('PRP'):\n",
    "        p+=1\n",
    "print(pn)\n",
    "print(n)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93fad966-a5d2-4247-98fc-1084875e4adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter statements: I am a girl. My name is ishita. I live in Bhopal. I study at banasthali vidyapith. I like food. How are you? Great! Eating, playing, happiness, fairly. She is a good girl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#WAP to count VERB vs AUX separately, and total “verb-like” count.\n",
    "ps=PorterStemmer()\n",
    "text=input(\"Enter statements:\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "tags=nltk.pos_tag(tokens)\n",
    "v=0\n",
    "a=0\n",
    "for w,t in tags:\n",
    "    if t.startswith('VB'):\n",
    "        if w in [\"is\",\"are\",\"was\",\"were\",\"am\"]:\n",
    "            a+=1\n",
    "        else:\n",
    "            v+=1\n",
    "   \n",
    "print(v)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c59b6bb-2982-4368-8ef7-84a9a9b0fc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter statements: I am a girl. My name is ishita. I live in Bhopal. I study at banasthali vidyapith. I like food. How are you? Great! Eating, playing, happiness, fairly. She is a good girl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 2), ('name', 1), ('bhopal', 1), ('banasthali', 1), ('vidyapith', 1), ('food', 1), ('eat', 1), ('play', 1), ('happi', 1)]\n",
      "[('is', 2), ('am', 1), ('live', 1), ('studi', 1), ('are', 1), ('great', 1)]\n"
     ]
    }
   ],
   "source": [
    "#WAP to print the top 10 most frequent nouns and top 10 most frequent verbs (by lemma, ignoring stopwords/punct).\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "text=input(\"Enter statements:\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "tags=nltk.pos_tag(tokens)\n",
    "noun_freq=Counter()\n",
    "verb_freq=Counter()\n",
    "for w,t in tags:\n",
    "    if t.startswith('NN'):\n",
    "        noun_freq[ps.stem(w)]+=1\n",
    "    elif t.startswith('VB'):\n",
    "        verb_freq[ps.stem(w)]+=1\n",
    "print(noun_freq.most_common(10))\n",
    "print(verb_freq.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15510aef-a991-48e3-8d1f-13fc799acf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter statements: I am a girl. My name is ishita. I live in Bhopal. I study at banasthali vidyapith. I like food. How are you? Great! Eating, playing, happiness, fairly. She is a good girl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage: 22.727272727272727\n"
     ]
    }
   ],
   "source": [
    "#WAP to compute the percentage of nouns in the text: denominator = total non-space, non-punct tokens\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "text=input(\"Enter statements:\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "tags=nltk.pos_tag(tokens)\n",
    "n=0\n",
    "total=0\n",
    "for w,t in tags:\n",
    "    total+=1\n",
    "    if t.startswith('NN'):\n",
    "        n+=1\n",
    "print(\"Percentage:\",(n/total)*100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ae944a8-64c7-4061-b3b2-abc87b85d4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter statements: I am a girl. My name is ishita. I live in Bhopal. I study at banasthali vidyapith. I like food. How are you? Great! Eating, playing, happiness, fairly. She is a good girl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "{'PRP', 'DT', '.', 'NN', 'JJ', 'IN', 'WRB', 'VBP', 'VB', 'NNP', 'PRP$', 'RB', 'VBZ', ','}\n"
     ]
    }
   ],
   "source": [
    "#WAP to count the number of different Penn POS tags present in the text using token.tag_ and print the set.\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "text=input(\"Enter statements:\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "tags=nltk.pos_tag(tokens)\n",
    "pos_tags=set(t for w, t in tags)\n",
    "print(len(pos_tags))\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d59952d-e035-4d95-928f-fd01757c5c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter statements: I am a girl. My name is ishita. I live in Bhopal. I study at banasthali vidyapith. I like food. How are you? Great! Eating, playing, happiness, fairly. She is a good girl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "{'ADV', '.', 'PRON', 'NOUN', 'DET', 'VERB', 'ADP', 'ADJ'}\n"
     ]
    }
   ],
   "source": [
    " #WAP to count the number of different Universal POS tags present and print the set.\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "text=input(\"Enter statements:\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "tags=nltk.pos_tag(tokens)\n",
    "pos_tags=set(nltk.map_tag(\"en-ptb\",\"universal\",t) for w,t in tags)\n",
    "print(len(pos_tags))\n",
    "print(pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c22d05cf-4267-4992-91fc-6d3fc6b05a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter statements: I am a girl. My name is ishita. I live in Bhopal. I study at banasthali vidyapith. I like food. How are you? Great! Eating, playing, happiness, fairly. She is a good girl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRON -> {'PRP$', 'PRP'}\n",
      "VERB -> {'VBP', 'VBZ', 'VB'}\n",
      "DET -> {'DT'}\n",
      "NOUN -> {'NNP', 'NN'}\n",
      ". -> {'.', ','}\n",
      "ADJ -> {'JJ'}\n",
      "ADP -> {'IN'}\n",
      "ADV -> {'WRB', 'RB'}\n"
     ]
    }
   ],
   "source": [
    "#WAP to print a mapping summary: for each Universal POS, list which Penn tags appeared under it (e.g., NOUN -> {NN, NNS, ...}).\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "text=input(\"Enter statements:\")\n",
    "tokens=nltk.word_tokenize(text)\n",
    "tags=nltk.pos_tag(tokens)\n",
    "mapping=defaultdict(set)\n",
    "for w,t in tags:\n",
    "    u=nltk.map_tag(\"en-ptb\",\"universal\",t)\n",
    "    mapping[u].add(t)\n",
    "for k in mapping:\n",
    "    print(k,\"->\",mapping[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb5dd1-2ebf-4c6b-99a0-3e3a8b1c14a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e92f8a6-668d-4549-bbc2-b99dde0d5e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87acf9a-ab4e-43a4-be0d-d491aff8991d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
